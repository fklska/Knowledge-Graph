# Knowledge-Graph
Описание индустриального проекта по построению графа знаний

1. [Предобработка источников](#Конвертация-документов-в-Markdown)
2. [Пайплайн извлечения сущностей](#LCEL-LangChain-Expression-Language)
3. [Организация базы данных](#Neo4j---Не-графом-единым)
4. LLM Inference и DRAGON

## Введение 
### Существующие подходы

MIT

From local to Global: GraphRAG

Prompt me one more time

## Конвертация документов в Markdown

В качестве входного текста было принято решение использовать `markdown` разметку. Обосновывается это тем, что LLM обучаются на огромных объёмах Markdown-подобных текстов (GitHub, документация, Wiki),
поэтому элементы `Markdown` разметки интерпретируются моделью как сигналы структуры, а не просто символы. Также использование размеченного текста помогает лучше его делить на смысловые части, отделя новые главы, параграфы и темы исходного источника.

В качестве основного метода конвертации документов в `markdown` были выбраны OCR модели. Они показывали хорошие результаты, по сравнению с алгоритмическими методами. Небольшой обзор по текущим OCR решениям можно посмотреть кликнув по картинке. Для нас было важно качественная конвертация таблиц, формул, списков, заголовков, в общем всех структурных элементов. Лучше всего зарекомендовали себя MinerU и Marker
<div>
  <a href="https://miro.com/app/board/uXjVJh4vnAs=/?moveToWidget=3458764655967474926&cot=14">
    <img width="1291" height="1209" alt="изображение" src="https://github.com/user-attachments/assets/d077f250-0df6-43cf-8262-18c84ace756f" />
  </a>
</div>


## LCEL-LangChain Expression Language

Библиотека `Langchain` предоставлет удобный функционал для работы с LLM, для извлечение сущностей была использована - *gpt-oss20b*. Весь пайплайн состоит из нескольких элементов:
1. `TextSplitter` - для деления входного текста на чанки, в нашем случае был использован `MarkdownTextSplitter`, регулируя параметры `chunk_size` и `chunk_overlap` можно менять количество чанков, на которые делится входной источник
2. Основная цепочка LCEL - состоит из: промпта, llm и выходного парсера. `Langchain` прекрасен тем что многие его компоненты основанны на классе `Runnable` и имеет метод `invoke()`.

`ChatPromptTemplate` требует входной запрос -> возвращает `PormptValue`

`ChatOpenAI` (LLM) требует на вход `PromptValue` -> возвращает `AIMessage`

`OutputParser` требует `AIMessage` -> возвращает `PydanticClass`

На основе этого строится основная обрабатывающая цепочка, на вход которой подается чанк исходного текста, и возвращает список сущностей в виде объекта `Python`. Этот процесс можно ускорить, используя асинхронность и метода `abatch`.

Цепочка inference, строится по такому же принципу, только на вход уже подается пользовательский запрос, а `retrival` происходит на уровне вызова инструментов LLM, по стандарту OpenAI API.

Все последующие вопросы нормализации сущностей, создания индексов, извлечения знаний решаются на уровне базы данных.

## Neo4j - Не графом единым

В качестве инструмента хранения и обработки графа используется база данных Neo4j.
### Нормализация
Нормализация сущностей основана на основе векторно сходства эмбеддингов выделенных LLM сущностей. Если значение косинусной близости эмбеддинга новой сущности и существующих больше некоторого порога, то сущности объединяются, иначе считается, что сущность несет в себе новую информацию. Благодаря этому можно регулировать степень контекста задавать его жестче или мягче. Например, спирт - бывает технический, а бывает медицинский. В контексте может упоминаться и тот и тот, но стоит ли их объединять или разделять зависит от конретной задачи. На основе этого изменяется некоторые свойства графа: количество компонентов связности и средняя степень ноды, что влияет на процесс `retrival` (извлечения).
<div>
  <a href="https://miro.com/app/board/uXjVJh4vnAs=/?moveToWidget=3458764655979610862&cot=10">
    <img width="2441" height="640" alt="изображение" src="https://github.com/user-attachments/assets/f5f6234d-a32c-4ff2-b91c-c5f32596bb16" />
  </a>
</div>

 

### Гибридный поиск
Реализация гибридного поиска основана на двух индексах: векторного и полнотекстового. Выглядит это примерно так:
<img width="720" height="480" alt="изображение" src="https://github.com/user-attachments/assets/af70da66-f79a-4779-8f37-fecd828bc916" />

*Источник: [DAT: Dynamic Alpha Tuning](https://arxiv.org/pdf/2503.23013)*

Выбираются первые k нод из векторного поиска и первые k нод которые вернул BM-25. Если ноды попали в оба индекса, то их `score` складывается; если нода только в одном индексе, то складывать не с чем. На основе этого выдаются первые n нод с максимальным `score`. При помощи параметра $alpha_{}$ можно регулировать силу поиска, путем умножения его на `score`, т.е. искать с большей основой на векторный поиск/bm25, или вовсе уйти в крайность искать только по bm25 при $alpha_{} = 0$. 

Интересная работа [DAT](https://arxiv.org/pdf/2503.23013) по динамическому регулированию параметра $alpha$, например если некторые запросы содержат имена собственные, то лучше искать по bm25, а если вопрос более общий то лучше по векторному, но пока без этих излишеств.

### Retrieval
При помощи гибридного поиска возвращает n наиболее релевантных нод. От этих нод запускается BFS (обход в ширину) на некоторую глубину $depth$ и возвращаются триплеты.
Извлеченные триплеты форматируются в некторую строку и подаются в prompt
```python
"""
Найденные сущности:
Name description
...

Найденные связи:
Name - relation - Name
...
"""
```

В ходе работы, изменения пайплайна и добавления фич, граф менялся следуюим образом:
<div>
  <a href="https://miro.com/app/board/uXjVJh4vnAs=/?moveToWidget=3458764655976379728&cot=14">
    <img width="1083" height="1442" alt="изображение" src="https://github.com/user-attachments/assets/f59da865-1e33-49e3-bba0-8824a0d54dcd" />
  </a>
</div>

