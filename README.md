# Knowledge-Graph
Описание индустриального проекта по построению графа знаний

1. [Предобработка источников](#Конвертация-документов-в-Markdown)
2. [Пайплайн извлечения сущностей](#LCEL-LangChain-Expression-Language)
3. [Организация базы данных](#Neo4j---Не-графом-единым)
4. [LLM Inference и DRAGON](#)

## Введение 
### Существующие подходы

MIT

From local to Global: GraphRAG

Prompt me one more time

## Конвертация документов в Markdown

В качестве входного текста было принято решение использовать `markdown` разметку. Обосновывается это тем, что LLM обучаются на огромных объёмах Markdown-подобных текстов (GitHub, документация, Wiki),
поэтому элементы `Markdown` разметки интерпретируются моделью как сигналы структуры, а не просто символы. Также использование размеченного текста помогает лучше его делить на смысловые части, отделя новые главы, параграфы и темы исходного источника.

В качестве основного метода конвертации документов в `markdown` были выбраны OCR модели. Они показывали хорошие результаты, по сравнению с алгоритмическими методами. Небольшой обзор по текущим OCR решениям можно посмотреть кликнув по картинке. Для нас было важно качественная конвертация таблиц, формул, списков, заголовков, в общем всех структурных элементов. Лучше всего зарекомендовали себя MinerU и Marker
<div>
  <a href="https://miro.com/app/board/uXjVJh4vnAs=/?moveToWidget=3458764655967474926&cot=14">
    <img width="1291" height="1209" alt="изображение" src="https://github.com/user-attachments/assets/d077f250-0df6-43cf-8262-18c84ace756f" />
  </a>
</div>


## LCEL-LangChain Expression Language

Библиотека `Langchain` предоставлет удобный функционал для работы с LLM, для извлечение сущностей была использована - *gpt-oss20b*. Весь пайплайн состоит из нескольких элементов:
1. `TextSplitter` - для деления входного текста на чанки, в нашем случае был использован `MarkdownTextSplitter`, регулируя параметры `chunk_size` и `chunk_overlap` можно менять количество чанков, на которые делится входной источник
2. Основная цепочка LCEL - состоит из: промпта, llm и выходного парсера. `Langchain` прекрасен тем что многие его компоненты основанны на классе `Runnable` и имеет метод `invoke()`.

`ChatPromptTemplate` требует входной запрос -> возвращает `PormptValue`

`ChatOpenAI` (LLM) требует на вход `PromptValue` -> возвращает `AIMessage`

`OutputParser` требует `AIMessage` -> возвращает `PydanticClass`

На основе этого строится основная обрабатывающая цепочка, на вход которой подается чанк исходного текста, и возвращает список сущностей в виде объекта `Python`. Этот процесс можно ускорить, используя асинхронность и метода `abatch`.

Цепочка inference, строится по такому же принципу, только на вход уже подается пользовательский запрос, а `retrival` происходит на уровне вызова инструментов LLM, по стандарту OpenAI API.

Все последующие вопросы нормализации сущностей, создания индексов, извлечения знаний решаются на уровне базы данных.

## Neo4j - Не графом единым

В качестве инструмента хранения и обработки графа используется база данных Neo4j.
### Нормализация
Нормализация сущностей основана на основе векторно сходства эмбеддингов выделенных LLM сущностей. Если значение косинусной близости эмбеддинга новой сущности и существующих больше некоторого порога, то сущности объединяются, иначе считается, что сущность несет в себе новую информацию. Благодаря этому можно регулировать степень контекста задавать его жестче или мягче. Например, спирт - бывает технический, а бывает медицинский. В контексте может упоминаться и тот и тот, но стоит ли их объединять или разделять зависит от конретной задачи. На основе этого изменяется некоторые свойства графа: количество компонентов связности и средняя степень ноды, что влияет на процесс `retrival` (извлечения).
<div>
  <a href="https://miro.com/app/board/uXjVJh4vnAs=/?moveToWidget=3458764655979610862&cot=10">
    <img width="2441" height="640" alt="изображение" src="https://github.com/user-attachments/assets/f5f6234d-a32c-4ff2-b91c-c5f32596bb16" />
  </a>
</div>

 

### Гибридный поиск
Реализация гибридного поиска основана на двух индексах: векторного и полнотекстового. Выглядит это примерно так:
<img width="720" height="480" alt="изображение" src="https://github.com/user-attachments/assets/af70da66-f79a-4779-8f37-fecd828bc916" />

*Источник: [DAT: Dynamic Alpha Tuning](https://arxiv.org/pdf/2503.23013)*

Выбираются первые k нод из векторного поиска и первые k нод которые вернул BM-25. Если ноды попали в оба индекса, то их `score` складывается; если нода только в одном индексе, то складывать не с чем. На основе этого выдаются первые n нод с максимальным `score`. При помощи параметра $alpha_{}$ можно регулировать силу поиска, путем умножения его на `score`, т.е. искать с большей основой на векторный поиск/bm25, или вовсе уйти в крайность искать только по bm25 при $alpha_{} = 0$. 

Интересная работа [DAT](https://arxiv.org/pdf/2503.23013) по динамическому регулированию параметра $alpha$, например если некторые запросы содержат имена собственные, то лучше искать по bm25, а если вопрос более общий то лучше по векторному, но пока без этих излишеств.

### Retrieval
При помощи гибридного поиска возвращает n наиболее релевантных нод. От этих нод запускается BFS (обход в ширину) на некоторую глубину $depth$ и возвращаются триплеты.
Извлеченные триплеты форматируются в некторую строку и подаются в prompt примерно в таком виде:
```python
"""
Найденные сущности:
Name description
...

Найденные связи:
Name - relation - Name
...
"""
```
## LLM Inference и DRAGON

Как я уже говорил выше цепочка inference, строиться по аналогии с построением графа, но с другим проптом и без выходного парсера. В PromptMessages закидывается запрос пользователя и информация, которую вернула база данных. Если использовать асинхронность, все выполняется достаточно оперативно. Ну и в добавок можно регуляровать параметры: 
- $alpha$ - сила поиска, векторный, полнотекствый или поровну
- $depth$ - глубина поиска, нужно быть с этим осторожным, потому что чем больше граф, тем больше число возвращаемых нод. По проведенным экспериментам выше 3 не имеет смылса, так как возвращает уже ноды из другой темы.
- $n$ - количество возвращаемых нод для bfs, если $depth$ - это глубина поиска, то $n$ - это обширность поиска. На практике лучше увеличивать $n$, чем $depth$
- $context-trsh$ - очень тонкий параметр при построении графа, если выбрать слишком маленький, разные ноды начнут объединятся и будет терятся информация, для бенчамрка использовался 0.985

Так как весь пайплайн был настроен на русский язык, то и бенчмарк должен был быть на русском. Было принято решение использовать [DRAGON](https://arxiv.org/pdf/2507.05713), дипломная работа студентов из ИТМО и не только. Есть текст новостей, вопросы и ответы к ним, в качестве метрик использовался rouge_score и оценка от LLM. Результаты получились следующие:<br>
<p>
Модель построения графа: gpt-oss-20b<br>
Модель ответа на вопросы: gpt-oss-20b<br>
Модель судья: qwen3-235b-a22b-2507<br>

Метрики:<br>
LLM as Jude : 0.48<br>
rouge1: 0.29<br>
rougeL: 0.27<br>
</p>
В оригинале этот бенчмарк оценивает классический RAG, поэтому получены не все метрики. Судя по Leaderboard модель на 20b параметров ответила не хуже чем Qwen 5-32b

Исходные результаты пожно глянуть по [ссылке](https://docs.google.com/spreadsheets/d/19vdhkvGBUYxaoxuE2z7h6o3zFuUvYqHY/edit?usp=sharing&ouid=112645934863507693718&rtpof=true&sd=true).

## Итого

Проект получился очень интересным и прикладным. Много вопросов и задач, которые удалось решить. Посмотрим, что будет дальше)

Все что я написал выше не было придумано сразу, а лишь дорабатывалось и изменялось в процессе изучения документации и отладки, поэтому можно глянуть эволюцию как менялся граф:
<div>
  <a href="https://miro.com/app/board/uXjVJh4vnAs=/?moveToWidget=3458764655976379728&cot=14">
    <img width="1083" height="1442" alt="изображение" src="https://github.com/user-attachments/assets/f59da865-1e33-49e3-bba0-8824a0d54dcd" />
  </a>
</div>

